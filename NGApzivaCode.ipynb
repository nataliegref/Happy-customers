{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5f70f7f7",
   "metadata": {},
   "source": [
    "Goal of exercise: predict with >73% accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "b1ac1eba",
   "metadata": {},
   "outputs": [],
   "source": [
    "#set up\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "\n",
    "# load dataset\n",
    "data = pd.read_csv('ACME-HappinessSurvey2020.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "800f685e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_logistic(feature_cols, data, plot = False):\n",
    "    \n",
    "    print('Logistic regression')\n",
    "    #set up data x and y\n",
    "    X = data[feature_cols]\n",
    "    y = data.Y \n",
    "    # # Split the data into training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=16)\n",
    "\n",
    "    # instantiate the model (using the default parameters)\n",
    "    logreg = LogisticRegression(random_state=16)\n",
    "\n",
    "    # fit the model with data\n",
    "    logreg.fit(X_train, y_train)\n",
    "\n",
    "    y_pred = logreg.predict(X_test)\n",
    "    cnf_matrix = metrics.confusion_matrix(y_test, y_pred)\n",
    "\n",
    "    if plot:\n",
    "        class_names=[0,1] # name  of classes\n",
    "        fig, ax = plt.subplots()\n",
    "        tick_marks = np.arange(len(class_names))\n",
    "        plt.xticks(tick_marks, class_names)\n",
    "        plt.yticks(tick_marks, class_names)\n",
    "        # create heatmap\n",
    "        sns.heatmap(pd.DataFrame(cnf_matrix), annot=True, cmap=\"YlGnBu\" ,fmt='g')\n",
    "        ax.xaxis.set_label_position(\"top\")\n",
    "        plt.tight_layout()\n",
    "        plt.title('Confusion matrix', y=1.1)\n",
    "        plt.ylabel('Actual label')\n",
    "        plt.xlabel('Predicted label')\n",
    "    else:\n",
    "        pass\n",
    "#         print('Confusion matrix:')\n",
    "#         print(cnf_matrix)\n",
    "    \n",
    "    #precision and accuracy\n",
    "\n",
    "    #target_names = ['unhappy', 'happy']\n",
    "    #print(classification_report(y_test, y_pred, target_names=target_names))\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "    print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f89f6700",
   "metadata": {},
   "source": [
    "Modifying logistic regression to improve accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "29cb6071",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_featureeng(feature_cols, data):\n",
    "    print('Logistic regression with feature engineering')\n",
    "    #set up data x and y\n",
    "    #feature engineering\n",
    "    poly = PolynomialFeatures(degree=2)\n",
    "    X = poly.fit_transform(data[feature_cols])\n",
    "    y = data.Y \n",
    "    # Split the data into training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=16)\n",
    "\n",
    "    # Train the Logistic Regression classifier\n",
    "    clf = LogisticRegression()\n",
    "    clf.fit(X_train, y_train)\n",
    "\n",
    "    y_pred = clf.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "    print(\"Accuracy:\", accuracy)\n",
    "    \n",
    "def run_featurescal(feature_cols, data):\n",
    "    print('Logistic regression with feature scaling')\n",
    "    #set up data x and y\n",
    "    #feature scaling\n",
    "    scaler = StandardScaler()\n",
    "    X = scaler.fit_transform(data[feature_cols])\n",
    "    y = data.Y \n",
    "    # Split the data into training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=16)\n",
    "\n",
    "    # Train the Logistic Regression classifier\n",
    "    clf = LogisticRegression()\n",
    "    clf.fit(X_train, y_train)\n",
    "\n",
    "    y_pred = clf.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "    print(\"Accuracy:\", accuracy)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "0753a3ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic regression\n",
      "Accuracy: 0.71875\n",
      "Logistic regression with feature engineering\n",
      "Accuracy: 0.59375\n",
      "Logistic regression with feature scaling\n",
      "Accuracy: 0.71875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "feature_cols = ['X1', 'X2', 'X3','X4','X5','X6']\n",
    "run_logistic(feature_cols, data)\n",
    "run_featureeng(feature_cols, data)\n",
    "run_featurescal(feature_cols, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "225bd187",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic regression accuracy: 0.71875\n",
      "KNN accuracy: 0.46875\n",
      "Decision tree accuracy: 0.5\n",
      "Random forest accuracy: 0.5625\n",
      "Gradient Boosting accuracy: 0.59375\n",
      "\n",
      "With grid search:\n",
      "Best hyperparameters for Logistic Regression (not scaled):  {'C': 0.1}\n",
      "Best accuracy score for Logistic Regression (not scaled):  0.5742690058479532\n",
      "Best hyperparameters for Logistic Regression (scaled):  {'C': 0.1}\n",
      "Best accuracy score for Logistic Regression (scaled):  0.5637426900584795\n",
      "Best hyperparameters for KNN (not scaled):  {'n_neighbors': 11}\n",
      "Best accuracy score for KNN (not scaled):  0.6052631578947368\n",
      "Best hyperparameters for KNN (scaled):  {'n_neighbors': 11}\n",
      "Best accuracy score for KNN (scaled):  0.6374269005847953\n",
      "Best hyperparameters for Decision Tree (not scaled):  {'max_depth': 3}\n",
      "Best accuracy score for Decision Tree (not scaled):  0.6169590643274854\n",
      "Best hyperparameters for Decision Tree (scaled):  {'max_depth': 3}\n",
      "Best accuracy score for Decision Tree (scaled):  0.6169590643274854\n"
     ]
    }
   ],
   "source": [
    "#comparing some other algorithms\n",
    "feature_cols = ['X1', 'X2', 'X3','X4','X5','X6']\n",
    "\n",
    "#Non scaled features\n",
    "X1 = data[feature_cols]\n",
    "# Scale the features using StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X2 = scaler.fit_transform(data[feature_cols])\n",
    "\n",
    "y = data.Y \n",
    "\n",
    "# Split the data into training and testing sets for scaled and non scaled\n",
    "X1_train, X1_test, y1_train, y1_test = train_test_split(X1, y, test_size=0.25, random_state=16)\n",
    "X2_train, X2_test, y2_train, y2_test = train_test_split(X2, y, test_size=0.25, random_state=16)\n",
    "\n",
    "# Define the hyperparameter grid to search over\n",
    "param_grid = {'C': [0.1, 1, 10, 100]}\n",
    "\n",
    "# Create a Logistic Regression classifier\n",
    "clf = LogisticRegression()\n",
    "clf.fit(X1_train, y1_train)\n",
    "y_pred = clf.predict(X1_test)\n",
    "accuracy = accuracy_score(y1_test, y_pred)\n",
    "print(f\"Logistic regression accuracy: {accuracy}\")\n",
    "\n",
    "clf_logistic = LogisticRegression()\n",
    "\n",
    "# Create a KNN classifier\n",
    "clf = KNeighborsClassifier()\n",
    "clf.fit(X1_train, y1_train)\n",
    "y_pred = clf.predict(X1_test)\n",
    "accuracy = accuracy_score(y1_test, y_pred)\n",
    "print(f\"KNN accuracy: {accuracy}\")\n",
    "\n",
    "clf_knn = KNeighborsClassifier()\n",
    "\n",
    "# Create a Decision Tree classifier\n",
    "clf = DecisionTreeClassifier()\n",
    "clf.fit(X1_train, y1_train)\n",
    "y_pred = clf.predict(X1_test)\n",
    "accuracy = accuracy_score(y1_test, y_pred)\n",
    "print(f\"Decision tree accuracy: {accuracy}\")\n",
    "\n",
    "clf_dt = DecisionTreeClassifier()\n",
    "\n",
    "# Create a Random Forest classifier\n",
    "clf = RandomForestClassifier()\n",
    "clf.fit(X1_train, y1_train)\n",
    "y_pred = clf.predict(X1_test)\n",
    "accuracy = accuracy_score(y1_test, y_pred)\n",
    "print(f\"Random forest accuracy: {accuracy}\")\n",
    "\n",
    "clf_rf = RandomForestClassifier()\n",
    "\n",
    "#Create a Gradient Boosting classifier\n",
    "clf = GradientBoostingClassifier()\n",
    "clf.fit(X1_train, y1_train)\n",
    "y_pred = clf.predict(X1_test)\n",
    "accuracy = accuracy_score(y1_test, y_pred)\n",
    "print(f\"Gradient Boosting accuracy: {accuracy}\")\n",
    "\n",
    "clf_gb = GradientBoostingClassifier()\n",
    "\n",
    "print(\"\\nWith grid search:\")\n",
    "#not scaled\n",
    "# Use GridSearchCV to perform a model selection\n",
    "grid = GridSearchCV(clf_logistic, param_grid, scoring='accuracy', cv=5)\n",
    "grid.fit(X1_train, y1_train)\n",
    "\n",
    "# Print the best hyperparameters and the best accuracy score\n",
    "print(\"Best hyperparameters for Logistic Regression (not scaled): \", grid.best_params_)\n",
    "print(\"Best accuracy score for Logistic Regression (not scaled): \", grid.best_score_)\n",
    "\n",
    "#scaled\n",
    "# Use GridSearchCV to perform a model selection\n",
    "grid = GridSearchCV(clf_logistic, param_grid, scoring='accuracy', cv=5)\n",
    "grid.fit(X2_train, y2_train)\n",
    "\n",
    "# Print the best hyperparameters and the best accuracy score\n",
    "print(\"Best hyperparameters for Logistic Regression (scaled): \", grid.best_params_)\n",
    "print(\"Best accuracy score for Logistic Regression (scaled): \", grid.best_score_)\n",
    "\n",
    "# Repeat the process for each classifier\n",
    "#not scaled\n",
    "grid = GridSearchCV(clf_knn, {'n_neighbors': range(1, 31)}, scoring='accuracy', cv=5)\n",
    "grid.fit(X1_train, y1_train)\n",
    "print(\"Best hyperparameters for KNN (not scaled): \", grid.best_params_)\n",
    "print(\"Best accuracy score for KNN (not scaled): \", grid.best_score_)\n",
    "\n",
    "#scaled\n",
    "grid = GridSearchCV(clf_knn, {'n_neighbors': range(1, 31)}, scoring='accuracy', cv=5)\n",
    "grid.fit(X2_train, y2_train)\n",
    "print(\"Best hyperparameters for KNN (scaled): \", grid.best_params_)\n",
    "print(\"Best accuracy score for KNN (scaled): \", grid.best_score_)\n",
    "\n",
    "#not scaled\n",
    "grid = GridSearchCV(clf_dt, {'max_depth': range(1, 11)}, scoring='accuracy', cv=5)\n",
    "grid.fit(X1_train, y1_train)\n",
    "print(\"Best hyperparameters for Decision Tree (not scaled): \", grid.best_params_)\n",
    "print(\"Best accuracy score for Decision Tree (not scaled): \", grid.best_score_)\n",
    "\n",
    "#scaled\n",
    "grid = GridSearchCV(clf_dt, {'max_depth': range(1, 11)}, scoring='accuracy', cv=5)\n",
    "grid.fit(X2_train, y2_train)\n",
    "print(\"Best hyperparameters for Decision Tree (scaled): \", grid.best_params_)\n",
    "print(\"Best accuracy score for Decision Tree (scaled): \", grid.best_score_)\n",
    "\n",
    "#not scaled\n",
    "grid = GridSearchCV(clf_rf, {'n_estimators': [10, 50, 100, 200], 'max_depth': range(1, 11)}, scoring='accuracy', cv=5)\n",
    "grid.fit(X1_train, y1_train)\n",
    "print(\"Best hyperparameters for Random Forest (not scaled): \", grid.best_params_)\n",
    "print(\"Best accuracy score for Random Forest (not scaled): \", grid.best_score_)\n",
    "\n",
    "#scaled\n",
    "grid = GridSearchCV(clf_rf, {'n_estimators': [10, 50, 100, 200], 'max_depth': range(1, 11)}, scoring='accuracy', cv=5)\n",
    "grid.fit(X2_train, y2_train)\n",
    "print(\"Best hyperparameters for Random Forest (scaled): \", grid.best_params_)\n",
    "print(\"Best accuracy score for Random Forest (scaled): \", grid.best_score_)\n",
    "\n",
    "#not scaled\n",
    "grid = GridSearchCV(clf_gb, {'n_estimators': [10, 50, 100, 200], 'max_depth': range(1, 11)}, scoring='accuracy', cv=5)\n",
    "grid.fit(X1_train, y1_train)\n",
    "print(\"Best hyperparameters for Gradient Boosting (not scaled): \", grid.best_params_)\n",
    "print(\"Best accuracy score for Gradient Boosting (not scaled): \", grid.best_score_)\n",
    "\n",
    "#scaled\n",
    "grid = GridSearchCV(clf_gb, {'n_estimators': [10, 50, 100, 200], 'max_depth': range(1, 11)}, scoring='accuracy', cv=5)\n",
    "grid.fit(X2_train, y2_train)\n",
    "print(\"Best hyperparameters for Gradient Boosting (scaled): \", grid.best_params_)\n",
    "print(\"Best accuracy score for Gradient Boosting(scaled): \", grid.best_score_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "83b43ff7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6875\n"
     ]
    }
   ],
   "source": [
    "def run_svc(feature_cols, data, plot = True):\n",
    "    #set up data x and y\n",
    "    X = data[feature_cols]\n",
    "    y = data.Y \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25)\n",
    "    # Train the SVM classifier\n",
    "    clf = SVC()\n",
    "    clf.fit(X_train, y_train)\n",
    "\n",
    "    # Predict the target values for the test set\n",
    "    y_pred = clf.predict(X_test)\n",
    "\n",
    "    # Calculate the accuracy of the model\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "    print(\"Accuracy:\", accuracy)\n",
    "\n",
    "feature_cols = ['X1', 'X2', 'X3','X4','X5','X6']\n",
    "run_svc(feature_cols, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "f11bea38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.5625\n"
     ]
    }
   ],
   "source": [
    "def run_NB(feature_cols, data, plot = True):\n",
    "    #set up data x and y\n",
    "    X = data[feature_cols]\n",
    "    y = data.Y \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25)\n",
    "\n",
    "    # Train the Gaussian Naive Bayes classifier\n",
    "    clf = GaussianNB()\n",
    "    clf.fit(X_train, y_train)\n",
    "\n",
    "    # Predict the target values for the test set\n",
    "    y_pred = clf.predict(X_test)\n",
    "\n",
    "    # Calculate the accuracy of the model\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "    print(\"Accuracy:\", accuracy)\n",
    "feature_cols = ['X1', 'X2', 'X3','X4','X5','X6']\n",
    "run_NB(feature_cols, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "3028c187",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.40625\n"
     ]
    }
   ],
   "source": [
    "def run_MLPC(feature_cols, data, plot = True):\n",
    "    #set up data x and y\n",
    "    X = data[feature_cols]\n",
    "    y = data.Y \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25)\n",
    "    # Train the Neural Network classifier\n",
    "    clf = MLPClassifier(hidden_layer_sizes=(10,), max_iter=1000)\n",
    "    clf.fit(X_train, y_train)\n",
    "\n",
    "    # Predict the target values for the test set\n",
    "    y_pred = clf.predict(X_test)\n",
    "    # Calculate the accuracy of the model\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "    print(\"Accuracy:\", accuracy)\n",
    "feature_cols = ['X1', 'X2', 'X3','X4','X5','X6']\n",
    "run_MLPC(feature_cols, data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1946214",
   "metadata": {},
   "source": [
    "The results show that using logistic regressin we can achieve highest accuracy but it is still below 73%"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bb013f9",
   "metadata": {},
   "source": [
    "Looking at individual features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "8048fccc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:xlabel='X1', ylabel='Count'>"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "happy = data[data['Y'] == 1]\n",
    "unhappy = data[data['Y'] == 0]\n",
    "# create distribution plot\n",
    "sns.histplot(data=happy, x='X1', kde=True)\n",
    "sns.histplot(data=unhappy, x='X1', kde=True, color = 'r')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "bbb4160b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Feature  Importance\n",
      "4      X5    2.273286\n",
      "0      X1    1.449041\n",
      "2      X3    0.900104\n",
      "5      X6    0.541070\n",
      "3      X4    0.106194\n",
      "1      X2    0.036161\n"
     ]
    }
   ],
   "source": [
    "#filter features by importance\n",
    "\n",
    "# Perform feature selection using the chi2 method\n",
    "selector = SelectKBest(score_func=chi2, k='all')\n",
    "selector.fit(X, y)\n",
    "\n",
    "# Get the feature importances based on the chi2 scores\n",
    "scores = selector.scores_\n",
    "\n",
    "# Create a dataframe with the feature names and their importances\n",
    "features = X.columns\n",
    "importances = pd.DataFrame({'Feature': features, 'Importance': scores})\n",
    "\n",
    "# Sort the dataframe by importance\n",
    "importances = importances.sort_values(by='Importance', ascending=False)\n",
    "\n",
    "# Print the feature importances\n",
    "print(importances)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d832668",
   "metadata": {},
   "source": [
    "Using SelectKBest and Chi squared as a feature selection approach we can see the order of importance of the questions.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "9887ae7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_features(features, data):\n",
    "    features.reverse()\n",
    "    for i in range(len(features)-1):\n",
    "        print(f\"Feature removed: {features[i]}\")\n",
    "        test_features = features[i+1:]\n",
    "        run_logistic(test_features, data, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "b626d89a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature removed: X2\n",
      "Accuracy: 0.6875\n",
      "Feature removed: X4\n",
      "Accuracy: 0.6875\n",
      "Feature removed: X6\n",
      "Accuracy: 0.65625\n",
      "Feature removed: X3\n",
      "Accuracy: 0.65625\n",
      "Feature removed: X1\n",
      "Accuracy: 0.625\n"
     ]
    }
   ],
   "source": [
    "features_byimp = list(importances.Feature) #removed X2 based on above results and rerun logisitc regression\n",
    "#run_logistic(new_feature_cols, data, plot = True)\n",
    "remove_features(features_byimp, data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae5b6c37",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f720eb3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
